{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASH analysis of GTEx data, Urbut 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs the analysis of GTEx data for Urbut et al 2017, using (by default) a prototype version of MASH implementation that produces results discussed in the manuscript.\n",
    "\n",
    "Note that:\n",
    "\n",
    "1. The input data has been converted from eQTL summary statistic to MASH format using a separate workflow (`fastqtl_to_mash.ipynb`). \n",
    "2. In this notebook we did not apply the inference to all the gene-snp pairs. Rather we focused on the \"top\" gene-snp pairs as a demonstration. It should be straightforward to configure the Posterior computatoin step to work on all gene-snp pairs instead. But we'd recommand using the `mashr` workflow which is a lot faster (see below)\n",
    "3. We have improved MASH implementation since the paper was submitted. See section \"Analysis with `mashr` package\" for the `mashr` workflow, and section \"Run this notebook\" about how to trigger this new workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this notebook\n",
    "For repeated runs it might be easier to execute from commandline instead of in Jupyter:\n",
    "\n",
    "```bash\n",
    "sos run workflows/gtex6_mash_analysis.ipynb # --data ... --cwd ...\n",
    "```\n",
    "\n",
    "The notebook runs default setting, ie, all the analysis steps. To view all available analysis:\n",
    "\n",
    "```bash\n",
    "sos run workflows/gtex6_mash_analysis.ipynb -h\n",
    "```\n",
    "\n",
    "Additionally I run it for dataset after LD pruning (for LD related discussion in supplemental information):\n",
    "\n",
    "```bash\n",
    "sos run workflows/gtex6_mash_analysis.ipynb --data data/MatrixEQTLSumStats.Portable.ld2.Z.rds\n",
    "```\n",
    "\n",
    "The outcome of this notebook should be found under `./gtex6_workflow_output` folder (can be configured), with the following output:\n",
    "\n",
    "```\n",
    "gtex6_mash_analysis.html\n",
    "MatrixEQTLSumStats.Portable.Z.sfa.rds.log\n",
    "MatrixEQTLSumStats.Portable.Z.sfa.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.rds\n",
    "pis.remove_before_rerun.pdf\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.pihat.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.loglik.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.posterior.rds\n",
    "```\n",
    "\n",
    "We keep track of results from every MASH step, though the inference of interest should be found in the `*.posterior.rds` file generated at the end of the pipeline.\n",
    "\n",
    "### A faster implementation via package `mashr`\n",
    "\n",
    "See section \"Analysis with `mashr` package\" and the `mashr` workflow for details. To use this workflow, add `--algorithm mashr` to the run commands above.\n",
    "\n",
    "If you run from the docker image `gaow/mash-paper` you need to put a copy of [MOSEK license file](https://www.mosek.com/products/academic-licenses) to `<workdir>/mosek.lic` (ie, `gtex6_workflow_output/mosek.lic` if you did not change any settings below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prototype (default) MASH implementation\n",
    "\n",
    "Some notes about the code used for the `mash` workflow analysis (some available from the `mash` workflow, some in the downloaded `mash_script_download` code):\n",
    "    \n",
    "    1. `ms=deconvolution.em.with.bovy(t.stat,factor.mat,v.j,lambda.mat,K=3,P=3)`\n",
    "\n",
    "produces an object with the denoised matrices for feeding into the\n",
    "*mash* covariance code. The *factor.mat* and *lambda.mat* called\n",
    "within have been produced by SFA and are single rank factors and\n",
    "loadings approximating the empirical covariance.\n",
    "\n",
    "    2. `covmat=compute.hm.covmat.all.max.step(b.hat=z.stat,se.hat=v.j,t.stat=z.stat,Q=5,lambda.mat,A=A,factor.mat,max.step=max.step,zero=TRUE)$covmat` \n",
    "\n",
    "produces a list of covariance matrices entitled *covmat\"A\".rds* upon\n",
    "which to base the mixture of multivariate normals.\n",
    "\n",
    "    3. `compute.hm.train.log.lik(train.b = train.z,se.train = train.v,covmat = covmat,A,pen=TRUE)`\n",
    "\n",
    "computes the HM weights on training datauses the set of randomly chosen genes to train our model and produces\n",
    "a matrix of likelihoods and corresponding hierarchical weights, as well as the mixture proportions.\n",
    "\n",
    "    4. `weightedquants=lapply(seq(1:nrow(z.stat)),function(j){total.quant.per.snp(j,covmat,b.gp.hat=z.stat,se.gp.hat = v.j,pis,A,checkpoint = FALSE)})`\n",
    "\n",
    "produces files containing the posterior means, upper and lower\n",
    "tail probabilities, null probabilites, and lfsr for all J genes across\n",
    "44 conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path('./gtex6_workflow_output')\n",
    "parameter: data = path(\"data/MatrixEQTLSumStats.Portable.Z.rds\")\n",
    "# path configured to /opt folder outside $HOME, to make it easier to use with `docker`\n",
    "parameter: mash_src = file_target(\"/opt/mash-paper/main.R\")\n",
    "parameter: sfa_exe = file_target(\"/opt/sfa/bin/sfa_linux\")\n",
    "sfa_data = file_target(f\"{cwd:a}/{data:bn}.sfa.rds\")\n",
    "# parameters for mashr analysis\n",
    "parameter: algorithm = 'paper'\n",
    "parameter: empirical_cov = 0\n",
    "parameter: vhat = 1\n",
    "mosek_license = file_target(f\"{cwd:a}/mosek.lic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance pattern discovery\n",
    "This obtains covariance matrices, ie, the priors, for `mash` model.\n",
    "\n",
    "### SFA\n",
    "We analyze data with SFA. The cell below downloads SFA software and run it on data with rank `K = 5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sfa_download: provides = sfa_exe]\n",
    "# Download / install SFA (no need if running from docker `gaow/mash-paper`)\n",
    "download: decompress = True, dest_dir = f'{sfa_exe:ad}'\n",
    "    http://stephenslab.uchicago.edu/assets/software/sfa/sfa1.0.tar.gz\n",
    "\n",
    "[sfa: provides = sfa_data]\n",
    "# Perform SFA analysis (time estimate: <1min)\n",
    "depends: sfa_exe\n",
    "K = 5\n",
    "tmpfile = path(f\"{cwd:a}/{data:bn}.max.txt\")\n",
    "input: f\"{data:a}\"\n",
    "output: sfa_data\n",
    "R: expand = \"${ }\", stdout = f\"{_output}.log\", workdir = cwd\n",
    "    z = readRDS(${_input:r})$test.z\n",
    "    write.table(z, ${tmpfile:r}, col.names=F,row.names=F)\n",
    "    cmd = paste0('${sfa_exe} -gen ${tmpfile} -g ', dim(z)[1], ' -n ', dim(z)[2], \n",
    "                 ' -k ${K} -iter 50 -rand 999 -o ${_output:bn}')\n",
    "    system(cmd)\n",
    "    saveRDS(list(F = read.table(\"${_output:n}_F.out\"),\n",
    "                lambda = read.table(\"${_output:n}_lambda.out\"),\n",
    "                sigma2 = read.table(\"${_output:n}_sigma2.out\"),\n",
    "                alpha = read.table(\"${_output:n}_alpha.out\")), ${_output:r})\n",
    "bash: workdir = cwd, expand = '${ }'\n",
    "    rm -f *{_F.out,_lambda.out,_sigma2.out,_alpha.out} ${tmpfile}\n",
    "    rm -r output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create and refine multi-rank covariance matrices\n",
    "Here we create 3 covariance matrices:\n",
    "\n",
    "* SFA (rank 5, previously computed)\n",
    "* SVD (rank 3, to be computed)\n",
    "* Empirical covariance\n",
    "\n",
    "and apply [Extreme Deconvolution](https://github.com/jobovy/extreme-deconvolution) to refine the matrices. We observed that Extreme Deconvolution perserves rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mash_scripts_download: provides = mash_src]\n",
    "# Download / install MASH scripts (no need if running from docker `gaow/mash-paper`)\n",
    "output: mash_src\n",
    "download: decompress = True, dest_dir = cwd\n",
    "    https://github.com/stephenslab/mashr-paper/archive/v0.2-1.zip\n",
    "bash: expand = True, workdir = cwd\n",
    "    mkdir -p {mash_src:ad}\n",
    "    cp mashr-paper-0.2-1/R/* {mash_src:ad} && rm -rf mashr-paper-0.2-1\n",
    "\n",
    "[mash-paper_1: shared = {'mash_input': '_input'}]\n",
    "# Compute data-driven prior matrices (time estimate: ~30min)\n",
    "depends: R_library(\"ExtremeDeconvolution\"), mash_src, sfa_data\n",
    "K = 3\n",
    "P = 3\n",
    "input: f\"{data:a}\"\n",
    "output: f\"{cwd:a}/{data:bn}.coved.K{K}.P{P}.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    setwd(${mash_src:dar})\n",
    "    ret = sapply(list.files(pattern = \"*.R\"), source, .GlobalEnv)\n",
    "    setwd(${cwd:ar})\n",
    "    dat = readRDS(${_input:r})\n",
    "    t.stat = dat$test.z\n",
    "    mean.mat = matrix(rep(0,ncol(t.stat)*nrow(t.stat)),ncol=ncol(t.stat),nrow=nrow(t.stat))\n",
    "    s.j = matrix(rep(1,ncol(t.stat)*nrow(t.stat)),ncol=ncol(t.stat),nrow=nrow(t.stat))\n",
    "    v.mat = dat$vhat\n",
    "    v.j=list()\n",
    "    for(i in 1:nrow(t.stat)){v.j[[i]]=v.mat}\n",
    "    K = ${K}\n",
    "    P = ${P}\n",
    "    R = ncol(t.stat)\n",
    "    sfa = readRDS(${sfa_data:r})\n",
    "    init.cov = init.covmat(t.stat=t.stat, factor.mat = as.matrix(sfa$F),lambda.mat = as.matrix(sfa$lambda), K=K,P=P)\n",
    "    init.cov.list = list()\n",
    "    for(i in 1:K){init.cov.list[[i]]=init.cov[i,,]}\n",
    "    projection = list();for(l in 1:nrow(t.stat)){projection[[l]]=diag(1,R)}\n",
    "    e = ExtremeDeconvolution::extreme_deconvolution(ydata=t.stat, ycovar=v.j,\n",
    "                                                      xamp=rep(1/K,K), xmean=mean.mat, xcovar=init.cov.list,\n",
    "                                                      fixmean=T, projection=projection, logfile=${_output:nr})\n",
    "    true.covs = array(dim=c(K,R,R))\n",
    "    for(i in 1:K){true.covs[i,,]=e$xcovar[[i]]}\n",
    "    saveRDS(list(true.covs=true.covs,pi=e$xamp), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Add in canonical and single-rank covariance matrices\n",
    "\n",
    "Now additionally we include 2 other types of covariance matrices:\n",
    "* canonical configurations (aka `bmalite`)\n",
    "* single rank SFA\n",
    "\n",
    "We also expand the list of matrices by grid. At the end of this step (cell below) we are ready to fit the mash model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mash-paper_2: shared = {'prior_matrices': '_output'}]\n",
    "# Add in canonical configurations and single rank SFA priors (time estimate: <1min)\n",
    "depends: sos_variable('mash_input'), sfa_data\n",
    "output: f\"{_input:n}.lite.single.expanded.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    setwd(${mash_src:dar})\n",
    "    ret = sapply(list.files(pattern = \"*.R\"), source, .GlobalEnv)\n",
    "    setwd(${cwd:ar})\n",
    "    dat = readRDS(${mash_input:r})\n",
    "    z.stat = dat$test.z\n",
    "    rownames(z.stat) = NULL\n",
    "    colnames(z.stat) = NULL\n",
    "    v.mat = dat$vhat\n",
    "    s.j = matrix(rep(1,ncol(z.stat)*nrow(z.stat)),ncol=ncol(z.stat),nrow=nrow(z.stat))\n",
    "    sfa = readRDS(${sfa_data:r})\n",
    "    res = compute.hm.covmat.all.max.step(b.hat=z.stat,se.hat=s.j,\n",
    "                                          t.stat=z.stat,Q=5,\n",
    "                                          lambda.mat=as.matrix(sfa$lambda),\n",
    "                                          A='.remove_before_rerun',\n",
    "                                          factor.mat=as.matrix(sfa$F),\n",
    "                                          max.step=readRDS(${_input:r}),\n",
    "                                          zero=TRUE)\n",
    "    saveRDS(res, ${_output:r})\n",
    "\n",
    "bash: workdir = cwd\n",
    "    rm -f *.remove_before_rerun.rds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit MASH mixture model\n",
    "Using a training set, the cell below computes the weights for input covariance matrices (priors) in MASH mixture. The output contains matrix of log-likelihoods as well as weights learned from the hierarchical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mash-paper_3]\n",
    "# Fit MASH mixture model (time estimate: ~2.5hr)\n",
    "depends: sos_variable('mash_input'), R_library(\"SQUAREM\")\n",
    "output: f\"{_input:n}.pihat.rds\", f\"{_input:n}.loglik.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    library(\"SQUAREM\")\n",
    "    setwd(${mash_src:dar})\n",
    "    ret = sapply(list.files(pattern = \"*.R\"), source, .GlobalEnv)\n",
    "    setwd(${cwd:ar})\n",
    "    dat = readRDS(${mash_input:r})\n",
    "    v.mat = dat$vhat\n",
    "    covmat = readRDS(${_input:r})$covmat\n",
    "    train.z = as.matrix(dat$train.z)\n",
    "    rownames(train.z) = NULL\n",
    "    colnames(train.z) = NULL\n",
    "    train.v = train.z/train.z\n",
    "    res = compute.hm.train.log.lik.pen.vmat(train.b=train.z,\n",
    "                                            covmat=covmat,\n",
    "                                            A='.remove_before_rerun', \n",
    "                                            pen=1,\n",
    "                                            train.s=train.v,\n",
    "                                            cormat=v.mat)\n",
    "    saveRDS(res$pis, ${_output[0]:r})\n",
    "    saveRDS(res$lik.mat, ${_output[1]:r})\n",
    "\n",
    "bash: workdir = cwd\n",
    "    rm -f *.remove_before_rerun.rds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior inference\n",
    "Applying hyperparameters learned from the training set to the test set, the cell below computes posterior quantities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mash-paper_4]\n",
    "# Posterior inference on the \"top\" set of gene-snp pairs (time estimate: ~3.5hr)\n",
    "depends: sos_variable('mash_input'), sos_variable('prior_matrices')\n",
    "output: f\"{_input[0]:nn}.posterior.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    setwd(${mash_src:dar})\n",
    "    ret = sapply(list.files(pattern = \"*.R\"), source, .GlobalEnv)\n",
    "    setwd(${cwd:ar})\n",
    "    dat = readRDS(${mash_input:r})\n",
    "    z.stat = dat$test.z\n",
    "    v.mat = dat$vhat\n",
    "    s.j = matrix(rep(1,ncol(z.stat)*nrow(z.stat)),ncol=ncol(z.stat),nrow=nrow(z.stat))\n",
    "    pis = readRDS(${_input[0]:r})$pihat\n",
    "    covmat = readRDS(${prior_matrices:r})$covmat\n",
    "    res = lapply(seq(1:nrow(z.stat)), function(j){\n",
    "        total.quant.per.snp.with.vmat(j=j, covmat=covmat, \n",
    "                                      b.gp.hat=z.stat, \n",
    "                                      cormat=v.mat, \n",
    "                                      se.gp.hat=s.j, \n",
    "                                      pis=pis, \n",
    "                                      A='remove_before_rerun', \n",
    "                                      checkpoint=TRUE)})\n",
    "    # data formatting.\n",
    "    out = do.call(Map, c(f = rbind, res))\n",
    "    saveRDS(out, ${_output:r})\n",
    "\n",
    "bash: workdir = cwd\n",
    "    rm -f *.remove_before_rerun.rds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now MASH analysis is complete. I will use a separate notebook to summarize, plot and visualize the result of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with `mashr` package\n",
    "\n",
    "Since the submission of Urbut 2017 we have improved implementation of MASH algorithm to produce an R package, [`mashr`](https://github.com/stephenslab/mashr). Major improvements relevant to Urbut 2017 are:\n",
    "\n",
    "1. Faster computation of likelihood and posterior matrices via matrix algebra tricks and a C++ implementation.\n",
    "2. Faster computation of MASH mixture via convex optimization.\n",
    "\n",
    "Below is the same workflow as `mash` workflow previously discussed, but implemented with `mashr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mash-fast_1: shared = {'mash_input': '_input'}]\n",
    "# Compute data-driven / canonical prior matrices (time estimate: <10min)\n",
    "depends: R_library(\"ExtremeDeconvolution\"), R_library(\"mashr@stephenslab/mashr\"), sfa_data\n",
    "K = 3 # as in mash paper\n",
    "P = 3 # as in mash paper\n",
    "input: f\"{data:a}\"\n",
    "output: f\"{cwd:a}/{data:bn}.cov{empirical_cov}.K{K}.P{P}.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    dat = readRDS(${_input:r})\n",
    "    sfa_data = readRDS(${sfa_data:r})\n",
    "    mash_data = mashr::mash_set_data(as.matrix(dat$test.z), matrix(1, nrow(dat$test.z), ncol(dat$test.z)))\n",
    "    sfa_res = as.matrix(sfa_data$lambda) %*% as.matrix(sfa_data$F)\n",
    "    # SFA matrices\n",
    "    U.sfa = c(mashr::cov_from_factors(as.matrix(sfa_data$F), \"sfa${K}\"), list(\"tSFA\" = t(sfa_res) %*% sfa_res / nrow(dat$test.z)))\n",
    "    # SVD matrices\n",
    "    U.pca = mashr::cov_pca(mash_data, ${P})\n",
    "    # Emperical data matrices\n",
    "    # `cov_ed` will take significantly longer when this empirical convariance matrix is added\n",
    "    D.center = apply(as.matrix(dat$test.z), 2, function(x) x - mean(x))\n",
    "    # Denoised data-driven matrices\n",
    "    U.dd = c(U.sfa, U.pca, if (${empirical_cov} == 1) list(\"XX\" = t(D.center) %*% D.center / nrow(dat$test.z)) else list())\n",
    "    U.ed = mashr::cov_ed(mash_data, U.dd)\n",
    "    # Canonical matrices\n",
    "    U.can = mashr::cov_canonical(mash_data)\n",
    "    saveRDS(list(Ulist = c(U.ed, U.can), DD_raw = U.dd), ${_output:r})\n",
    "\n",
    "[mash-fast_2]\n",
    "# Fit MASH mixture model (time estimate: <5min)\n",
    "depends: R_library(\"REBayes\")\n",
    "output: f\"{_input:n}.V{vhat}.mash_model.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd, env = {'MOSEKLM_LICENSE_FILE': str(mosek_license)}\n",
    "    dat = readRDS(${mash_input:r})\n",
    "    null.z = rbind(dat$train.z, dat$validate.z)\n",
    "    if (${vhat}) {\n",
    "        V = cor(null.z[which(apply(abs(null.z),1, max) < 2),])\n",
    "    } else {\n",
    "        V = diag(ncol(null.z))\n",
    "    }\n",
    "    mash_data = mashr::mash_set_data(as.matrix(null.z), matrix(1, nrow(null.z), ncol(null.z)), V=as.matrix(V))\n",
    "    saveRDS(mashr::mash(mash_data, Ulist = readRDS(${_input:r})$Ulist, outputlevel = 1), ${_output:r})\n",
    "  \n",
    "[mash-fast_3]\n",
    "# Posterior inference on the \"top\" set of gene-snp pairs (time estimate: <3min)\n",
    "output: f\"{_input:n}.posterior.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    dat = readRDS(${mash_input:r})\n",
    "    null.z = rbind(dat$train.z, dat$validate.z)\n",
    "    if (${vhat}) {\n",
    "        V = cor(null.z[which(apply(abs(null.z),1, max) < 2),])\n",
    "    } else {\n",
    "        V = diag(ncol(null.z))\n",
    "    }\n",
    "    mash_data = mashr::mash_set_data(as.matrix(dat$test.z), matrix(1, nrow(dat$test.z), ncol(dat$test.z)), V=as.matrix(V))\n",
    "    saveRDS(mashr::mash_compute_posterior_matrices(readRDS(${_input:r}), mash_data), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export and run default pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[export]\n",
    "# Export notebook to HTML file\n",
    "input: [item for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 1\n",
    "output: [(f'{cwd:a}/{item:bn}.full.html', f'{cwd:a}/{item:bn}.lite.html') for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 2\n",
    "bash: expand = True, stderr = False\n",
    "  sos convert {_input} {_output[0]}\n",
    "  sos convert {_input} {_output[1]} --template sos-report\n",
    "  \n",
    "[default]\n",
    "# Run all analysis in this notebook\n",
    "if algorithm == 'paper':\n",
    "    sos_run('export+mash-paper')\n",
    "else:\n",
    "    fail_if(not mosek_license.is_file(), msg = f'Please put a valid copy (NOT a symbolic link!) of MOSEK license to: \\n``{mosek_license}``')\n",
    "    sos_run('export+mash-fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p class=\"session_section\">SoS</p>\n",
       "<table class=\"session_info\">\n",
       "<tr>\n",
       "<th>SoS Version</th><td><pre>0.9.13.3</pre></td>\n",
       "</tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sessioninfo"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": false,
    "height": 0,
    "style": "side"
   },
   "version": "0.9.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
