{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate adaptive shrinkage (MASH) analysis of GTEx data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code used to generate results for the Urbut *et al* (2017) manuscript.\n",
    "\n",
    "Important notes:\n",
    "\n",
    "1. Although you may open this notebook in Jupyter, you should not step through the code sequentially as you would in a typical Jupyter notebook; this is because the code in this notebook is meant to be run using the [Script of Scripts (SoS)](https://github.com/vatlab/SoS)\n",
    "framework.\n",
    "\n",
    "2. This notebook is not only used to reproduce Urbut 2017, but with the `--data` option you can provide your own data-set and perform MASH analysis. Here the input data have already been converted from the original eQTL summary statistics (downloaded from the GTEx Portal) to a format convenient for analysis in MASH. For details on this step, see the README and the `fastqtl_to_mash.ipynb` notebook. \n",
    "\n",
    "3. In this notebook, we did not apply the inference to all the gene-snp pairs. Rather we focused on the \"top\" gene-snp pairs as a demonstration. It should be straightforward to configure the Posterior computatoin step to work on all gene-snp pairs instead. But we'd recommand using the `mashr` workflow which is a lot faster (see below).\n",
    "\n",
    "4. We have improved MASH implementation since the paper was submitted. See section \"Analysis with `mashr` package\" for the `mashr` workflow, and section \"Run this notebook\" about how to trigger this new workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MASH as implemented in Urbut 2017\n",
    "For repeated runs it might be easier to execute from commandline instead of in Jupyter:\n",
    "\n",
    "```bash\n",
    "sos run workflows/gtex6_mash_analysis.ipynb # --data ... --cwd ...\n",
    "```\n",
    "\n",
    "The notebook runs default setting, ie, all the analysis steps. To view all available analysis:\n",
    "\n",
    "```bash\n",
    "sos run workflows/gtex6_mash_analysis.ipynb -h\n",
    "```\n",
    "\n",
    "Additionally I run it for dataset after LD pruning (for LD related discussion in supplemental information):\n",
    "\n",
    "```bash\n",
    "sos run workflows/gtex6_mash_analysis.ipynb --data data/MatrixEQTLSumStats.Portable.ld2.Z.rds\n",
    "```\n",
    "\n",
    "The outcome of this notebook should be found under `./gtex6_workflow_output` folder (can be configured), with the following output:\n",
    "\n",
    "```\n",
    "gtex6_mash_analysis.html\n",
    "MatrixEQTLSumStats.Portable.Z.sfa.rds.log\n",
    "MatrixEQTLSumStats.Portable.Z.sfa.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.pihat.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.loglik.rds\n",
    "MatrixEQTLSumStats.Portable.Z.coved.K3.P3.lite.single.expanded.posterior.rds\n",
    "```\n",
    "\n",
    "We keep track of results from every MASH step, though the inference of interest should be found in the `*.posterior.rds` file generated at the end of the pipeline.\n",
    "\n",
    "### Analysis Steps\n",
    "\n",
    "The pipeline automatically executes the following:\n",
    "\n",
    "+ Compute a sparse factorization of the (centered) z-scores using the\n",
    "  [SFA software](http://stephenslab.uchicago.edu/software.html#sfa),\n",
    "  with K = 5 factors, and save the factors in an `.rds` file. This\n",
    "  will be used to construct the mixture-of-multivariate normals\n",
    "  prior. This step is labeled `sfa`, and should only take a few\n",
    "  minutes to run.\n",
    "\n",
    "+ Compute additional \"data-driven\" prior matrices by computing a\n",
    "  singular value decomposition of the (centered) z-scores and low-rank\n",
    "  approximations to the empirical covariance matrices. Most of the\n",
    "  work in this step involves running the Extreme Deconvolution\n",
    "  method. The outcome of running the Extreme Deconvolution method is\n",
    "  saved to a new `.rds` file. This step is labeled `mash-paper_1` and\n",
    "  may take several hours to run (in one run on a MacBook Pro with\n",
    "  a 3.5 GHz Intel Core i7, it took over 6 hours to complete).\n",
    "\n",
    "+ A final collection of \"canonical\" and single-rank prior matrices\n",
    "  based on SFA and the \"BMAlite\" models of Flutre *et al*\n",
    "  (2013). These matrices are again written to another `.rds` file. This\n",
    "  step is labeled `mash-paper_2`, and should take at most a minute to\n",
    "  run.\n",
    "\n",
    "+ The `mash-paper_3` step fits the MASH (\"multivariate adaptive\n",
    "  shrinkage\") model to the GTEx data (the centered z-scores); the\n",
    "  model parameters estimated in this fitting step are the weights of\n",
    "  the multivariate normal mixture. The outputs from this step are the\n",
    "  estimated mixture weights and the conditional likelihood\n",
    "  matrix. These two outputs are saved to two separate `.rds` files.\n",
    "  This step is expected to take at most a few hours to complete.\n",
    "\n",
    "+ The `mash-paper_4` step computes posterior statistics using the\n",
    "  fitted MASH model from the previous step. These posterior statistics\n",
    "  are summarized and visualized in subsequent analyses. The posterior\n",
    "  statistics are saved to another `.rds` file. This step is expected\n",
    "  to take a few hours to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A faster implementation via package `mashr`\n",
    "\n",
    "See section \"Analysis with `mashr` package\" and the `mashr` workflow for details. To use this workflow, add `--algorithm mashr` to the run commands above.\n",
    "\n",
    "If you run from the docker image `gaow/mash-paper` you need to put a copy of [MOSEK license file](https://www.mosek.com/products/academic-licenses) to `<workdir>/mosek.lic` (ie, `gtex6_workflow_output/mosek.lic` if you did not change any settings below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prototype (default) MASH implementation\n",
    "\n",
    "Some notes about the code used for the `mash` workflow analysis (some available from the `mash` workflow, some in the downloaded `mash_script_download` code):\n",
    "    \n",
    "    1. `deconvolution.em.with.bovy(...)`\n",
    "\n",
    "produces an object with the denoised matrices for feeding into the\n",
    "*mash* covariance code. The *factor.mat* and *lambda.mat* called\n",
    "within have been produced by SFA and are single rank factors and\n",
    "loadings approximating the empirical covariance.\n",
    "\n",
    "    2. `compute.hm.covmat.all.max.step(...)$covmat` \n",
    "\n",
    "produces a list of covariance matrices entitled *covmat\"A\".rds* upon\n",
    "which to base the mixture of multivariate normals.\n",
    "\n",
    "    3. `compute.hm.train.log.lik(...)`\n",
    "\n",
    "computes the HM weights on training datauses the set of randomly chosen genes to train our model and produces\n",
    "a matrix of likelihoods and corresponding hierarchical weights, as well as the mixture proportions.\n",
    "\n",
    "    4. `total.quant.per.snp(...)`\n",
    "\n",
    "produces files containing the posterior means, upper and lower\n",
    "tail probabilities, null probabilites, and lfsr for all J genes across\n",
    "44 conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[global]\n",
    "parameter: cwd = path('./gtex6_workflow_output')\n",
    "parameter: data = path(\"data/MatrixEQTLSumStats.Portable.Z.rds\")\n",
    "# path configured to /opt folder outside $HOME, to make it easier to use with `docker`\n",
    "parameter: mash_src = file_target(\"/opt/mash-paper/main.R\")\n",
    "parameter: sfa_exe = file_target(\"/opt/sfa/bin/sfa_linux\")\n",
    "# parameters for mashr analysis\n",
    "parameter: algorithm = 'paper'\n",
    "parameter: vhat = 1\n",
    "mosek_license = file_target(f\"{cwd:a}/mosek.lic\")\n",
    "sfa_data = file_target(f\"{cwd:a}/{data:bn}.sfa.rds\")\n",
    "flash_data = file_target(f\"{cwd:a}/{data:bn}.flash.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance pattern discovery\n",
    "This obtains covariance matrices, ie, the priors, for `mash` model.\n",
    "\n",
    "### SFA\n",
    "We analyze data with SFA. The cell below downloads SFA software and run it on data with rank `K = 5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[sfa_download: provides = sfa_exe]\n",
    "# Download / install SFA (no need if running from docker `gaow/mash-paper`)\n",
    "download: decompress = True, dest_dir = f'{sfa_exe:ad}'\n",
    "    http://stephenslab.uchicago.edu/assets/software/sfa/sfa1.0.tar.gz\n",
    "\n",
    "[sfa: provides = sfa_data]\n",
    "# Perform SFA analysis (time estimate: <1min)\n",
    "depends: sfa_exe\n",
    "K = 5\n",
    "tmpfile = path(f\"{cwd:a}/{data:bn}.max.txt\")\n",
    "input: f\"{data:a}\"\n",
    "output: sfa_data\n",
    "R: expand = \"${ }\", stdout = f\"{_output:n}.log\", workdir = cwd\n",
    "    z = readRDS(${_input:r})$test.z\n",
    "    write.table(z, ${tmpfile:r}, col.names=F,row.names=F)\n",
    "    cmd = paste0('${sfa_exe} -gen ${tmpfile} -g ', dim(z)[1], ' -n ', dim(z)[2], \n",
    "                 ' -k ${K} -iter 50 -rand 999 -o ${_output:bn}')\n",
    "    system(cmd)\n",
    "    saveRDS(list(F = read.table(\"${_output:n}_F.out\"),\n",
    "                lambda = read.table(\"${_output:n}_lambda.out\"),\n",
    "                sigma2 = read.table(\"${_output:n}_sigma2.out\"),\n",
    "                alpha = read.table(\"${_output:n}_alpha.out\")), ${_output:r})\n",
    "bash: workdir = cwd, expand = '${ }'\n",
    "    rm -f *{_F.out,_lambda.out,_sigma2.out,_alpha.out} ${tmpfile}\n",
    "    rm -r output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create and refine multi-rank covariance matrices\n",
    "Here we create 3 covariance matrices:\n",
    "\n",
    "* SFA (rank 5, previously computed)\n",
    "* SVD (rank 3, to be computed)\n",
    "* Empirical covariance\n",
    "\n",
    "and apply [Extreme Deconvolution](https://github.com/jobovy/extreme-deconvolution) to refine the matrices. We observed that Extreme Deconvolution perserves rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[mash_scripts_download: provides = mash_src]\n",
    "# Download / install MASH scripts (no need if running from docker `gaow/mash-paper`)\n",
    "output: mash_src\n",
    "download: decompress = True, dest_dir = cwd\n",
    "    https://github.com/stephenslab/mashr-paper/archive/v0.2-1.zip\n",
    "bash: expand = True, workdir = cwd\n",
    "    mkdir -p {mash_src:ad}\n",
    "    cp mashr-paper-0.2-1/R/* {mash_src:ad} && rm -rf mashr-paper-0.2-1\n",
    "\n",
    "[mash-paper_1: shared = {'mash_input': '_input'}]\n",
    "# Compute data-driven prior matrices \n",
    "# (time estimate: 40min to 4hrs depending on the machine power)\n",
    "depends: R_library(\"ExtremeDeconvolution\"), mash_src, sfa_data\n",
    "K = 3\n",
    "P = 3\n",
    "input: f\"{data:a}\"\n",
    "output: f\"{cwd:a}/{data:bn}.coved.K{K}.P{P}.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    library(ExtremeDeconvolution)\n",
    "    script.source = sapply(list.files(${mash_src:dar}, pattern = \"*.R\", full.names=TRUE), source, .GlobalEnv)\n",
    "    z.stat = readRDS(${_input:r})$test.z\n",
    "    sfa = readRDS(${sfa_data:r})\n",
    "    s.j = matrix(rep(1,ncol(z.stat)*nrow(z.stat)),ncol=ncol(z.stat),nrow=nrow(z.stat))\n",
    "    res = deconvolution.em.with.bovy(z.stat, \n",
    "                                      as.matrix(sfa$F), \n",
    "                                      s.j, \n",
    "                                      as.matrix(sfa$lambda),\n",
    "                                      K = ${K}, P = ${P})\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in canonical and single-rank covariance matrices\n",
    "\n",
    "Now additionally we include 2 other types of covariance matrices:\n",
    "* canonical configurations (aka `bmalite`)\n",
    "* single rank SFA\n",
    "\n",
    "We also expand the list of matrices by grid. At the end of this step (cell below) we are ready to fit the mash model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[mash-paper_2: shared = {'prior_matrices': '_output'}]\n",
    "# Add in canonical configurations and single rank SFA priors (time estimate: <1min)\n",
    "depends: sos_variable('mash_input'), sfa_data\n",
    "output: f\"{_input:n}.lite.single.expanded.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    script.source = sapply(list.files(${mash_src:dar}, pattern = \"*.R\", full.names=TRUE), source, .GlobalEnv)\n",
    "    z.stat = readRDS(${mash_input:r})$test.z\n",
    "    rownames(z.stat) = NULL\n",
    "    colnames(z.stat) = NULL\n",
    "    s.j = matrix(rep(1,ncol(z.stat)*nrow(z.stat)),ncol=ncol(z.stat),nrow=nrow(z.stat))\n",
    "    sfa = readRDS(${sfa_data:r})\n",
    "    res = compute.hm.covmat.all.max.step(b.hat=z.stat,se.hat=s.j,\n",
    "                                          t.stat=z.stat,Q=5,\n",
    "                                          lambda.mat=as.matrix(sfa$lambda),\n",
    "                                          A='.${_input:bn}.remove_before_rerun',\n",
    "                                          factor.mat=as.matrix(sfa$F),\n",
    "                                          max.step=readRDS(${_input:r}),\n",
    "                                          zero=TRUE)\n",
    "    saveRDS(res$covmat, ${_output:r})\n",
    "\n",
    "bash: workdir = cwd, expand = True\n",
    "    rm -f *.{_input:bn}.remove_before_rerun.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit MASH mixture model\n",
    "Using a training set, the cell below computes the weights for input covariance matrices (priors) in MASH mixture. The output contains matrix of log-likelihoods as well as weights learned from the hierarchical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[mash-paper_3]\n",
    "# Fit MASH mixture model (time estimate: ~2.5hr)\n",
    "depends: sos_variable('mash_input'), R_library(\"SQUAREM\")\n",
    "output: f\"{_input:n}.V{vhat}.pihat.rds\", f\"{_input:n}.V{vhat}.loglik.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    library(\"SQUAREM\")\n",
    "    script.source = sapply(list.files(${mash_src:dar}, pattern = \"*.R\", full.names=TRUE), source, .GlobalEnv)\n",
    "    dat = readRDS(${mash_input:r})\n",
    "    covmat = readRDS(${_input:r})\n",
    "    train.z = as.matrix(dat$train.z)\n",
    "    rownames(train.z) = NULL\n",
    "    colnames(train.z) = NULL\n",
    "    train.v = matrix(rep(1,ncol(train.z)*nrow(train.z)),ncol=ncol(train.z),nrow=nrow(train.z))\n",
    "    res = compute.hm.train.log.lik.pen.vmat(train.b=train.z,\n",
    "                                            covmat=covmat,\n",
    "                                            cormat=${\"dat$vhat\" if vhat else \"diag(nrow(dat$vhat))\"},\n",
    "                                            A='.${_output[0]:bnn}.remove_before_rerun', \n",
    "                                            pen=TRUE,\n",
    "                                            train.s=train.v)\n",
    "    saveRDS(res$pis, ${_output[0]:r})\n",
    "    saveRDS(res$lik.mat, ${_output[1]:r})\n",
    "\n",
    "bash: workdir = cwd, expand = True\n",
    "    rm -f *.{_output[0]:bnn}.remove_before_rerun.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior inference\n",
    "Applying hyperparameters learned from the training set to the test set, the cell below computes posterior quantities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[mash-paper_4]\n",
    "# Posterior inference on the \"top\" set of gene-snp pairs \n",
    "# (time estimate: ~5hr on single thread)\n",
    "depends: sos_variable('mash_input'), sos_variable('prior_matrices')\n",
    "output: f\"{_input[0]:nn}.posterior.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    script.source = sapply(list.files(${mash_src:dar}, pattern = \"*.R\", full.names=TRUE), source, .GlobalEnv)\n",
    "    dat = readRDS(${mash_input:r})\n",
    "    z.stat = dat$test.z\n",
    "    s.j = matrix(rep(1,ncol(z.stat)*nrow(z.stat)),ncol=ncol(z.stat),nrow=nrow(z.stat))\n",
    "    pis = readRDS(${_input[0]:r})$pihat\n",
    "    covmat = readRDS(${prior_matrices:r})\n",
    "    res = lapply(seq(1:nrow(z.stat)), function(j){\n",
    "          total.quant.per.snp.with.vmat(j=j, \n",
    "                                      covmat=covmat, \n",
    "                                      b.gp.hat=z.stat,\n",
    "                                      se.gp.hat=s.j, \n",
    "                                      cormat=${\"dat$vhat\" if vhat else \"diag(nrow(dat$vhat))\"},\n",
    "                                      pis=pis, \n",
    "                                      A='', \n",
    "                                      checkpoint=TRUE)})\n",
    "    # data formatting.\n",
    "    out = do.call(Map, c(f = rbind, res))\n",
    "    saveRDS(out, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now MASH analysis is complete. I will use separate vignettes to summarize, plot and visualize the result of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with `mashr` package\n",
    "\n",
    "Since the submission of Urbut 2017 we have improved implementation of MASH algorithm to produce an R package, [`mashr`](https://github.com/stephenslab/mashr). Major improvements compared to Urbut 2017 are:\n",
    "\n",
    "1. Faster computation of likelihood and posterior matrices via matrix algebra tricks and a C++ implementation.\n",
    "2. Faster computation of MASH mixture via convex optimization.\n",
    "3. Add in `FLASH` method for prior covariance matrices.\n",
    "\n",
    "Below is the same workflow as `mash` workflow previously discussed, but implemented with `mashr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLASH analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[flash: provides = flash_data]\n",
    "# Perform FLASH analysis (time estimate: 20min)\n",
    "depends: R_library(\"mashr@stephenslab/flashr\"), R_library('mclust'), R_library('plyr')\n",
    "K = 5 # flash factors\n",
    "input: f\"{data:a}\"\n",
    "output: flash_data\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    get_flash_prior = function(test.z, K = ${K}, factor_plot = \"${_output:n}.factors.pdf\") {\n",
    "        Z.center = apply(test.z, 2, function(x) x - mean(x))\n",
    "        flash.data = flashr::flash_set_data(Z.center)\n",
    "        fmodel = flashr::flash(flash.data, greedy = TRUE, backfit = TRUE)\n",
    "        factors = flashr::flash_get_ldf(fmodel)$f\n",
    "        row.names(factors) = colnames(test.z)\n",
    "        pve.order = order(flashr::flash_get_pve(fmodel), decreasing = TRUE)\n",
    "        if (!is.null(factor_plot)) {\n",
    "            pdf(factor_plot)\n",
    "            #par(mar=c(.5,.5,.5,.5))\n",
    "            #par(mfrow=c(ceiling(length(pve.order) / 2), 2))\n",
    "            for(i in pve.order){\n",
    "              barplot(factors[,i], main=paste0('Factor ',i, ' pve= ', round(flashr::flash_get_pve(fmodel)[i],3)), las=2, cex.names = 0.7)\n",
    "            }\n",
    "            dev.off()\n",
    "          }\n",
    "        # flash on the loading\n",
    "        loading = fmodel$EL[,1:K]\n",
    "        colnames(loading) = paste0('Factor',seq(1,K))\n",
    "        flash.loading = flashr::flash_set_data(loading)\n",
    "        flmodel = flashr::flash(flash.loading, greedy = TRUE, backfit = TRUE)\n",
    "        # Cluster loadings\n",
    "        library(mclust)\n",
    "        mod = Mclust(loading)\n",
    "        U_list = plyr::alply(mod$parameters$variance$sigma,3)\n",
    "        mu_list = plyr::alply(mod$parameters$mean,2)\n",
    "        ll = list()\n",
    "        for (i in 1:length(U_list)){\n",
    "          ll[[i]] = U_list[[i]] + mu_list[[i]] %*% t(mu_list[[i]])\n",
    "        }\n",
    "        Factors = fmodel$EF[,1:5]\n",
    "        U.loading = lapply(ll, function(U){Factors %*% (U %*% t(Factors))})\n",
    "        names(U.loading) = paste0('Load', K, \"_\", (1:length(U.loading)))\n",
    "        # rank 1\n",
    "        Flash_res = flashr::flash_get_lf(fmodel)\n",
    "        U.flash = c(mashr::cov_from_factors(t(as.matrix(factors)), \"Flash\"),\n",
    "                    list(\"tFlash\" = t(Flash_res) %*% Flash_res / nrow(Z.center)))\n",
    "        return(list(U.loading = U.loading, U.flash = U.flash))\n",
    "    }\n",
    "    #\n",
    "    res = get_flash_prior(readRDS(${_input:r})$test.z)\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MASH analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[mash-fast_1: shared = {'mash_output': '_output'}]\n",
    "# Compute data-driven / canonical prior matrices (time estimate: ~20min)\n",
    "depends: R_library(\"ExtremeDeconvolution\"), R_library(\"mashr@stephenslab/mashr\"), flash_data, sfa_data\n",
    "parameter: alpha = 1\n",
    "K = 3 # as in mash paper\n",
    "P = 3 # as in mash paper\n",
    "input: f\"{data:a}\"\n",
    "output: f\"{cwd:a}/{data:bn}.V{vhat}.K{K}.P{P}.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    dat = readRDS(${_input:r})\n",
    "    null.z = rbind(dat$train.z, dat$validate.z)\n",
    "    if (${vhat}) {\n",
    "        V = as.matrix(cor(null.z[which(apply(abs(null.z),1, max) < 2),]))\n",
    "    } else {\n",
    "        V = as.matrix(diag(ncol(null.z)))\n",
    "    }\n",
    "    mash_data = mashr::mash_set_data(as.matrix(dat$test.b), Shat=as.matrix(dat$test.s), alpha=${alpha}, V=V)\n",
    "    sfa_data = readRDS(${sfa_data:r})\n",
    "    sfa_res = as.matrix(sfa_data$lambda) %*% as.matrix(sfa_data$F)\n",
    "    # SFA matrices\n",
    "    U.sfa = c(mashr::cov_from_factors(as.matrix(sfa_data$F), \"sfa${K}\"), list(\"tSFA\" = t(sfa_res) %*% sfa_res / nrow(dat$test.z)))\n",
    "    # FLASH matrices\n",
    "    flash_res = readRDS(${flash_data:r})\n",
    "    # SVD matrices\n",
    "    U.pca = mashr::cov_pca(mash_data, ${P})\n",
    "    # D.center is used to compute emperical cov matrix\n",
    "    D.center = apply(as.matrix(dat$test.z), 2, function(x) x - mean(x))\n",
    "    # Denoised data-driven matrices\n",
    "    U.dd = c(U.sfa, flash_res$U.flash, flash_res$U.loading, U.pca, list(\"XX\" = t(D.center) %*% D.center / nrow(dat$test.z)))\n",
    "    U.ed = mashr::cov_ed(mash_data, U.dd)\n",
    "    # Canonical matrices\n",
    "    U.can = mashr::cov_canonical(mash_data)\n",
    "    saveRDS(list(Ulist = c(U.ed, U.can), DD_raw = U.dd, V = V), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[mash-fast_2]\n",
    "# Fit MASH mixture model (time estimate: <5min)\n",
    "depends: R_library(\"REBayes\")\n",
    "output: f\"{_input:n}.mash_model.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd, env = {'MOSEKLM_LICENSE_FILE': str(mosek_license)}\n",
    "    dat = readRDS(${data:ar})\n",
    "    settings = readRDS(${_input:r})\n",
    "    train.b = rbind(dat$train.b, dat$validate.b)\n",
    "    train.s = rbind(dat$train.s, dat$validate.s)\n",
    "    mash_data = mashr::mash_set_data(as.matrix(dat$train.b), Shat=as.matrix(dat$train.s), alpha=${alpha}, V=settings$V)\n",
    "    saveRDS(mashr::mash(mash_data, Ulist = settings$Ulist, outputlevel = 1), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[mash-fast_3]\n",
    "# Posterior inference on the \"top\" set of gene-snp pairs (time estimate: <3min)\n",
    "output: f\"{_input:n}.posterior.rds\"\n",
    "R: expand = \"${ }\", workdir = cwd\n",
    "    dat = readRDS(${data:ar})\n",
    "    settings = readRDS(${mash_output:r})\n",
    "    mash_data = mashr::mash_set_data(as.matrix(dat$test.b), Shat=as.matrix(dat$test.s), alpha=${alpha}, V=settings$V)\n",
    "    saveRDS(mashr::mash_compute_posterior_matrices(readRDS(${_input:r}), mash_data), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export and run default pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[export]\n",
    "# Export notebook to HTML file\n",
    "input: [item for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 1\n",
    "output: [(f'{cwd:a}/{item:bn}.full.html', f'{cwd:a}/{item:bn}.lite.html') for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 2\n",
    "bash: expand = True, stderr = False\n",
    "  sos convert {_input} {_output[0]}\n",
    "  sos convert {_input} {_output[1]} --template sos-report\n",
    "  \n",
    "[default]\n",
    "# Run all analysis in this notebook\n",
    "sos_run('export')\n",
    "if algorithm == 'paper':\n",
    "    sos_run('mash-paper')\n",
    "else:\n",
    "    fail_if(not mosek_license.is_file(), msg = f'Please put a valid copy (NOT a symbolic link!) of MOSEK license to: \\n``{mosek_license}``')\n",
    "    sos_run('mash-fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p class=\"session_section\">SoS</p>\n",
       "<table class=\"session_info\">\n",
       "<tr>\n",
       "<th>SoS Version</th><td><pre>0.9.14.2</pre></td>\n",
       "</tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sessioninfo"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.9.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
