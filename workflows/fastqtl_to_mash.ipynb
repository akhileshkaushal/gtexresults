{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eQTL summary statistics formatting\n",
    "\n",
    "This workflow converts `fastqtl` eQTL analysis summary statistics text output to formats more friendly to R analysis. In particular:\n",
    "\n",
    "1. It converst single study results to HDF5 format grouped by genes.\n",
    "2. It combines multiple studies into one single HDF5 file. In the context of GTEx each study is result from one tissue.\n",
    "3. For MASH analysis in particular, it extracts from the complete data a subset of results to compute data driven MASH prior covariance, and to train the MASH mixture model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "### A list of summary statistics\n",
    "\n",
    "Summary statistics from `fasteqtl` are in text format, one row per gene-snp pair. Columns are:\n",
    "\n",
    "```\n",
    "gene_id \n",
    "variant_id      \n",
    "tss_distance    \n",
    "ma_samples      \n",
    "ma_count        \n",
    "maf     \n",
    "pval_nominal    \n",
    "slope   \n",
    "slope_se\n",
    "```\n",
    "\n",
    "Each analysis (tissue for GTEx) has a separate text file. Additionally there are support files of gene transcription start site coordinates, and SNP coordinates.\n",
    "\n",
    "The workflow takes a list of summary statistics file names, eg, `data/eQTLDataDemo/FastQTLSumStats.list` (can be configured) that has the contents:\n",
    "\n",
    "```\n",
    "Tissue_1.fastqtl.gz\n",
    "Tissue_2.fastqtl.gz\n",
    "...\n",
    "```\n",
    "\n",
    "The first two columns of these files have to be `gene_id` and `variant_id` (column name does not matter). For other contents we only need columns $\\hat{\\beta}$, $\\text{SE}(\\hat{\\beta})$ and p-value for the summary statistics. In the `fastqtl` output format above it is columns `8, 9, 7`. If your summary statistics file has a different format you can use `--cols` parameter to pass the proper column numbers (current default to `--cols 8 9 7`). **Currently this pipeline only supports and requires summary statistics $\\hat{\\beta}$, $\\text{SE}(\\hat{\\beta})$ and p-value, not other quantities (eg t statistic)**.\n",
    "\n",
    "### A list of gene names (optional)\n",
    "\n",
    "To speed up merging multiple HDF5 files it helps to provide a list of gene names. Otherwise it takes too much time to figure them out from individual HDF5 files before merger can happen. The gene list has the contents like:\n",
    "\n",
    "```\n",
    "ENSG00000186092.4\n",
    "ENSG00000227232.5\n",
    "ENSG00000228463.9\n",
    "ENSG00000241860.6\n",
    "ENSG00000268903.1\n",
    "ENSG00000269981.1\n",
    "ENSG00000279457.4\n",
    "ENSG00000279928.2\n",
    "...\n",
    "```\n",
    "\n",
    "## Run analysis\n",
    "\n",
    "Under the same folder as this list file, you keep all these listed data files. Then you run:\n",
    "\n",
    "```\n",
    "sos run workflows/fastqtl_to_mash.ipynb convert \\\n",
    "    --data_list data/eQTLDataDemo/FastQTLSumStats.list \\\n",
    "    --gene_list data/eQTLDataDemo/GTEx_genes.txt\n",
    "```\n",
    "to convert to HDF5 only, and \n",
    "\n",
    "```\n",
    "sos run workflows/fastqtl_to_mash.ipynb \\\n",
    "    --data_list data/eQTLDataDemo/FastQTLSumStats.list \\\n",
    "    --gene_list data/eQTLDataDemo/GTEx_genes.txt\n",
    "```\n",
    "\n",
    "to convert to HDF5 AND extract MASH input.\n",
    "\n",
    "## Other parameters\n",
    "\n",
    "- `--msg`\n",
    "- `--maxsize`\n",
    "- `--cols`\n",
    "- `--keep_ensg_version`\n",
    "- `--null_per_gene`\n",
    "- `--include_mash_effects_list`\n",
    "- `--include_mash_condition_list`\n",
    "- `--mash_train_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output data\n",
    "\n",
    "If you run the entire workflow, you should find under `./gtex6_workflow_output` (can be configured):\n",
    "\n",
    "- Study (tissue) specific HDF5 files of summary statistics\n",
    "- Merged HDF5 from multiple studies\n",
    "- \"*.portable.h5\" data extracted for MASH computations\n",
    "- \"*.mash.rds\" data in RDS format splitted to training/validation/testing/, with Z-scores computed for MASH EZ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path('./gtex6_workflow_output')\n",
    "parameter: data_list = path(\"data/eQTL_summary_files.txt\")\n",
    "parameter: gene_list = path()\n",
    "parameter: msg = \"GTEx eQTL summary statistics\"\n",
    "# maximum number of groups per HDF5 file\n",
    "parameter: maxsize = 1000 \n",
    "parameter: cols = [8, 9, 7]\n",
    "parameter: keep_ensg_version = 0\n",
    "# number of null samples per gene to train MASH mixture\n",
    "parameter: null_per_gene = 9 \n",
    "# a list of effect names (SNP names) to include in mash analysis\n",
    "parameter: include_mash_effects_list = path('NULL')\n",
    "# a list of condition names (tissue names) to include in mash analysis \n",
    "# Default includes all conditions.\n",
    "parameter: include_mash_condition_list = path('NULL')\n",
    "# size of MASH train set\n",
    "parameter: mash_train_size = 20000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5 utility codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[convert_0, default_0]\n",
    "# Generate utility functions\n",
    "depends: Py_Module('tables')\n",
    "report: expand = \"${ }\", output = '.sos/utils.py'\n",
    "    import sys, os, re, copy\n",
    "    import numpy as np, pandas as pd, tables as tb\n",
    "    tb.parameters.MAX_GROUP_WIDTH = 51200\n",
    "    # tb.parameters.NODE_CACHE_SLOTS = -51200\n",
    "    # tb.parameters.METADATA_CACHE_SIZE = 1048576 * 100000\n",
    "    # tb.parameters.CHUNK_CACHE_SIZE = 2097152 * 100000\n",
    "    # tb.parameters.CHUNK_CACHE_NELMTS = 521\n",
    "\n",
    "    class Environment:\n",
    "        def __init__(self):\n",
    "            self.float = np.float32\n",
    "            self.duplicate_tag = '_duplicated_'\n",
    "            self.common_suffix = '.h5'\n",
    "\n",
    "    env = Environment()\n",
    "\n",
    "    class TBData(dict):\n",
    "        def __init__(self, data, name, msg = None, root = '/', complib = 'bzip2'):\n",
    "            '''bzip2 may not be compatible with other hdf5 applications; but zlib is fine'''\n",
    "            self.__root = root.strip('/')\n",
    "            self.__group = name\n",
    "            self.__msg = msg\n",
    "            try:\n",
    "                if type(data) is dict:\n",
    "                    self.update(data)\n",
    "                elif type(data) is str:\n",
    "                    # is file name\n",
    "                    self.__load(tb.open_file(data))\n",
    "                else:\n",
    "                    # is file stream\n",
    "                    self.__load(data)\n",
    "            except tb.exceptions.NoSuchNodeError:\n",
    "                raise ValueError('Cannot find dataset {}!'.format(name))\n",
    "            self.tb_filters = tb.Filters(complevel = 9, complib=complib)\n",
    "\n",
    "        def sink(self, filename):\n",
    "            with tb.open_file(filename, 'a') as f:\n",
    "                if self.__root:\n",
    "                    try:\n",
    "                        f.create_group(\"/\", self.__root)\n",
    "                    except:\n",
    "                        pass\n",
    "                try:\n",
    "                    # there is existing data -- have to merge with current data\n",
    "                    # have to do this because the input file lines are not grouped by gene names!!\n",
    "                    # use try ... except to hopefully faster than if ... else\n",
    "                    # e.g., if not f.__contains__('/{}'.format(self.__group)) ... else ...\n",
    "                    for element in f.list_nodes('/{}/{}'.format(self.__root, self.__group)):\n",
    "                        if element.name != 'colnames':\n",
    "                            self[element.name] = np.concatenate((element[:], self[element.name]))\n",
    "                except tb.exceptions.NoSuchNodeError:\n",
    "                    f.create_group(\"/\" + self.__root, self.__group,\n",
    "                                   self.__msg if self.__msg else self.__group)\n",
    "                for key in self:\n",
    "                    self.__store_array(key, f)\n",
    "                f.flush()\n",
    "\n",
    "        def dump(self, table, output = False):\n",
    "            if output:\n",
    "                pd.DataFrame({self['colnames'][i] : self[table][:,i] for i in range(len(self['colnames']))}, index = self['rownames']).to_csv(sys.stdout, na_rep = 'NA')\n",
    "                return None\n",
    "            else:\n",
    "                return pd.DataFrame({self['colnames'][i] : self[table][:,i] for i in range(len(self['colnames']))}, index = self['rownames'])\n",
    "\n",
    "        def __load(self, fstream):\n",
    "            try:\n",
    "                for element in fstream.list_nodes('/{}/{}'.format(self.__root, self.__group)):\n",
    "                    self[element.name] = element[:]\n",
    "                fstream.close()\n",
    "            except:\n",
    "                fstream.close()\n",
    "                raise\n",
    "\n",
    "        def __roll_back(self, group, name):\n",
    "            try:\n",
    "                n = getattr(group, name)\n",
    "                n._f_remove()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        def __store_array(self, name, fstream):\n",
    "            if self.__root:\n",
    "                element = getattr(getattr(fstream.root, self.__root), self.__group)\n",
    "            else:\n",
    "                element = getattr(fstream.root, self.__group)\n",
    "            arr = self[name]\n",
    "            if type(arr) is list:\n",
    "                arr = np.array(arr)\n",
    "            self.__roll_back(element, name)\n",
    "            #\n",
    "            if arr.shape != (0,):\n",
    "                ds = fstream.create_carray(element, name, tb.Atom.from_dtype(arr.dtype), arr.shape,\n",
    "                                           filters = self.tb_filters)\n",
    "                ds[:] = arr\n",
    "\n",
    "    def get_tb_grps(filenames, group_name = None):\n",
    "        if isinstance(filenames, str):\n",
    "            filenames = [filenames]\n",
    "        names = set()\n",
    "        for filename in filenames:\n",
    "            with tb.open_file(filename) as f:\n",
    "                names.update([node._v_name for node in (f.root if group_name is None else getattr(f.root, '{}'.format(group_name)))])\n",
    "        return sorted(names)\n",
    "\n",
    "    class SSData:\n",
    "        def __init__(self, header = False):\n",
    "            self.data = {'buffer':{'data':[], 'rownames':[]}, 'output':{}}\n",
    "            self.header = header\n",
    "            self.previous_name = self.current_name = None\n",
    "            self.count = -1\n",
    "\n",
    "        def parse(self, line, ensg_version = 0):\n",
    "            # input line is snp, gene, beta, t, pval\n",
    "            if not line:\n",
    "                self.__reset()\n",
    "                self.current_name = None\n",
    "                return 1\n",
    "            if isinstance(line, bytes):\n",
    "                line = line.decode()\n",
    "            line = line.strip().split()\n",
    "            self.count += 1\n",
    "            if self.header and self.count == 0:\n",
    "                return 0\n",
    "            #\n",
    "            line[0] = line[0].strip('\"')\n",
    "            if ensg_version == 0:\n",
    "                line[0] = line[0].split('.')[0]\n",
    "            if self.previous_name is None:\n",
    "                self.previous_name = line[0]\n",
    "            self.current_name = line[0]\n",
    "            if self.current_name != self.previous_name:\n",
    "                self.__reset()\n",
    "            self.data['buffer']['data'].append([line[${cols[0]-1}], line[${cols[1]-1}], line[${cols[2]-1}]])\n",
    "            self.data['buffer']['rownames'].append(self.__format_variant_id(line[1]))\n",
    "            return 0\n",
    "        \n",
    "        @staticmethod\n",
    "        def __format_variant_id(value):\n",
    "            value = value.strip('\"').split('_')\n",
    "            if len(value) > 4:\n",
    "                # keep it chr, pos, ref, alt\n",
    "                value = value[:4]\n",
    "            if value[0].startswith('chr'):\n",
    "                value[0] = value[0][3:]\n",
    "            return '_'.join(value)\n",
    "\n",
    "        def __reset(self):\n",
    "            self.data['buffer']['data'] = np.array(self.data['buffer']['data'], dtype = env.float)\n",
    "            self.data['buffer']['rownames'] = np.array(self.data['buffer']['rownames'])\n",
    "            self.data['buffer']['colnames'] = np.array(['beta','se','pval'])\n",
    "            self.data['output'] = copy.deepcopy(self.data['buffer'])\n",
    "            self.data['buffer'] = {'data':[], 'rownames':[]}\n",
    "\n",
    "        def dump(self):\n",
    "            return self.data['output']\n",
    "\n",
    "    class DataMerger(TBData):\n",
    "        def __init__(self, files, name, msg = None):\n",
    "            TBData.__init__(self, {}, name, msg, complib = \"zlib\")\n",
    "            self.files = sorted(files)\n",
    "            self.__group = name\n",
    "\n",
    "        def merge(self):\n",
    "            data = {}\n",
    "            one_snp = None\n",
    "            failure_ct = 0\n",
    "            # Collect data\n",
    "            for item in self.files:\n",
    "                tissue = re.sub(r'{}$'.format(env.common_suffix), '', os.path.basename(item))\n",
    "                try:\n",
    "                    data[tissue] = TBData(item, self.__group)\n",
    "                    if one_snp is None: one_snp = data[tissue]['rownames'][0]\n",
    "                except ValueError:\n",
    "                    data[tissue] = {'data' : np.array([[np.nan, np.nan, np.nan]]), 'rownames': None}\n",
    "                    failure_ct += 1\n",
    "                # Fix row name\n",
    "                # Because in GTEx data file there are duplicated gene-snp pairs having different sumstats!!\n",
    "                if data[tissue]['rownames'] is not None:\n",
    "                    data[tissue]['rownames'] = self.__dedup(data[tissue]['rownames'], item)\n",
    "            if failure_ct == len(self.files):\n",
    "                return 1\n",
    "            # Merge data\n",
    "            for idx, item in enumerate(['beta','se','pval']):\n",
    "                self[item] = pd.concat([pd.DataFrame(\n",
    "                    {tissue : data[tissue]['data'][:,idx]},\n",
    "                    index = data[tissue]['rownames'] if data[tissue]['rownames'] is not None else [one_snp]\n",
    "                    ) for tissue in sorted(data.keys())], axis = 1)\n",
    "                if 'rownames' not in self:\n",
    "                    self['rownames'] = np.array(self[item].index, dtype = str)\n",
    "                if 'colnames' not in self:\n",
    "                    self['colnames'] = np.array(self[item].columns.values.tolist(), dtype = str)\n",
    "                self[item] = np.array(self[item].as_matrix(), dtype = env.float)\n",
    "            # np.savetxt(sys.stdout, self['pval'], fmt='%10.5f')\n",
    "            # print(self['rownames'])\n",
    "            # print(self['colnames'])\n",
    "            return 0\n",
    "\n",
    "        def __dedup(self, seq, filename):\n",
    "            seen = {}\n",
    "            dups = set()\n",
    "            def __is_seen(x, seen):\n",
    "                if x not in seen:\n",
    "                    seen[x] = 0\n",
    "                    return 0\n",
    "                else:\n",
    "                    seen[x] += 1\n",
    "                    dups.add(x)\n",
    "                    return 1\n",
    "            # Tag them\n",
    "            obs = [x if not __is_seen(x, seen) else '%s%s%s' % (x, env.duplicate_tag, seen[x]) for x in seq]\n",
    "            # Log them\n",
    "            if len(dups):\n",
    "                filename = os.path.splitext(filename)[0]\n",
    "                with open(filename + '.error', 'a') as f:\n",
    "                    for item in dups:\n",
    "                        f.write('{}:{} appeared {} times in {}\\n'.\\\n",
    "                                format(self.__group, item, seen[item] + 1, filename))\n",
    "            return obs\n",
    "\n",
    "    def get_gs_pairs(data, name, num = (1, 9), method = 'equal_space'):\n",
    "        '''choose gene-snp pairs from data, controlled by num = (a, b)\n",
    "        for the best gene-snp pair (a = 0 or 1), and b other random \n",
    "        gene-snp pairs'''\n",
    "\n",
    "        def random_sample(x, k):\n",
    "            return sorted(set(np.random.choice(x, min(len(x), k))))\n",
    "\n",
    "        def equal_sample(x, k):\n",
    "            if len(x) < k:\n",
    "                return x\n",
    "            f = lambda m, n: [i*n//m + n//(2*m) for i in range(m)]\n",
    "            return sorted(set([x[i] for i in f(k, len(x))]))\n",
    "\n",
    "        output = {'colnames' : data['colnames']}\n",
    "        lp = data.dump('pval')\n",
    "        lp = lp[np.all(np.isfinite(lp), axis=1)]\n",
    "        #\n",
    "        if lp.empty:\n",
    "            return None\n",
    "        # Find max SNP-gene pair\n",
    "        lp = -np.log10(lp)\n",
    "        rowidx = np.where(data['rownames'] == lp.max(axis=1).idxmax())[0][0]\n",
    "        if num[0] > 0:\n",
    "            output['max_rownames'] = ['%s_%s' % (name, data['rownames'][rowidx].decode())]\n",
    "            output['max'] = {}\n",
    "            for k in ['beta', 'pval', 'se']:\n",
    "                output['max'][k] = data[k][rowidx, :]\n",
    "        if num[1] > 0:\n",
    "            all_nullidxes = [y for y, x in enumerate(data['rownames']) if x in lp.index and y != rowidx]\n",
    "            sample_nullidxes = randome_sample(all_nullidxes, num[1]) if method == 'random' else equal_sample(all_nullidxes, num[1])\n",
    "            output['null_rownames'] = ['%s_%s' % (name, data['rownames'][x].decode()) for x in sample_nullidxes]\n",
    "            output['null'] = {}\n",
    "            for k in ['beta', 'pval', 'se']:\n",
    "                output['null'][k] = data[k][sample_nullidxes, :]\n",
    "        if not 'max' in output and not 'null' in output:\n",
    "            output = None\n",
    "        return output\n",
    "\n",
    "    def merge_tmp_h5(output, verbose = 0):\n",
    "        from glob import glob\n",
    "        tmpfiles = list(glob(output + \"_*.tmp\"))\n",
    "        if os.path.isfile(output+'.h5'):\n",
    "            os.remove(output+'.h5')\n",
    "        for item in sorted(tmpfiles):\n",
    "            for name in get_tb_grps(item):\n",
    "                cmd = 'h5copy -i {0} -o {2} -s \"/{1}\" -d \"/{1}\"'.format(item, name, output+'.h5')\n",
    "                if verbose:\n",
    "                    print(cmd)\n",
    "                os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to HDF5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Per study (tissue) conversion\n",
    "\n",
    "For per study conversion I use `bzip2` compression method and `float32` to achieve higher compression rate. This will generate one HDF5 file per summary statistics file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[convert_1, default_1]\n",
    "# Convert summary stats gzip format to HDF5\n",
    "depends: executable(\"h5copy\")\n",
    "fail_if(not data_list.is_file(), msg = 'Need data list file!')\n",
    "data_files = set(get_output(f\"awk '{{print $1}}' {data_list:e}\").strip().split('\\n'))\n",
    "fail_if(len(data_files) == 0, msg = 'Need input data files!')\n",
    "input: [f'{data_list:d}/{x}' for x in data_files], group_by = 1, concurrent = True\n",
    "output: f'{cwd:a}/{_input:bn}.h5'\n",
    "task: concurrent = True\n",
    "\n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp\n",
    "    \n",
    "python: expand = \"${ }\", input = '.sos/utils.py'\n",
    "    import warnings\n",
    "    import gzip\n",
    "    ssp = SSData(header = True)\n",
    "    group_counts = 0\n",
    "    with gzip.open(${_input:r}) as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            quit = ssp.parse(line, ${keep_ensg_version})\n",
    "            if ssp.current_name != ssp.previous_name:\n",
    "                group_counts += 1\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\", category = tb.FlavorWarning)\n",
    "                    data = TBData(ssp.dump(), ssp.previous_name, \"${msg}\")\n",
    "                    data.sink(\"${_output:n}_%i.tmp\" % (np.ceil(group_counts / ${maxsize})) \n",
    "                                if ${maxsize} > 0 else ${_output:r})\n",
    "                ssp.previous_name = ssp.current_name\n",
    "            if quit:\n",
    "                break\n",
    "    if ${maxsize} > 0:\n",
    "        merge_tmp_h5(${_output:nr})\n",
    "\n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge per study (tissue) HDF5 to one HDF5\n",
    "Create tables for each summary statistic per gene from multiple studies. Rows are SNPs, columns are tissue names. This time I use `zlib` compression for better compatibility with other HDF5 tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[convert_2, default_2]\n",
    "# Merge single study data to multivariate data\n",
    "depends: executable(\"h5copy\")\n",
    "output: f'{cwd:a}/{data_list:bn}.h5'\n",
    "task:\n",
    "\n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp\n",
    "\n",
    "python: expand = '${ }', input = '.sos/utils.py'\n",
    "    import warnings\n",
    "    if ${gene_list.is_file()}:\n",
    "        gene_names = [x.strip() for x in open(${gene_list:r}).readlines() if x.strip()]\n",
    "    else:\n",
    "        gene_names = get_tb_grps([${_input:r,}])\n",
    "    if ${keep_ensg_version} == 0:\n",
    "        gene_names = [os.path.splitext(x)[0] for x in gene_names]\n",
    "    failure_ct = 0\n",
    "    for idx, item in enumerate(gene_names):\n",
    "        ssm = DataMerger([${_input:r,}], item, \"${msg}\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category = tb.FlavorWarning)\n",
    "            if ssm.merge() == 0:\n",
    "                ssm.sink(\"${_output:n}_%i.tmp\" % (np.ceil((idx + 1.0) / ${maxsize})) \n",
    "                        if ${maxsize} > 0 else ${_output:r})\n",
    "            else:\n",
    "                failure_ct += 1\n",
    "    with open(\"${_output:n}.log\", 'w') as f:\n",
    "        f.write(\"%s out of %s groups merged!\\n\" % (len(gene_names) - failure_ct, len(gene_names)))\n",
    "    if ${maxsize} > 0:\n",
    "        merge_tmp_h5(${_output:nr})\n",
    "            \n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data for MASH model\n",
    "\n",
    "We need to extract the \"top\" signals based on single tissue analysis, as well as some null data as training / testing sets, to compute MASH priors and fit MASH mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_3]\n",
    "# Extract data to fit MASH model\n",
    "output: f\"{_input:n}.portable.h5\"\n",
    "task:\n",
    "\n",
    "python: expand = \"${ }\", input = '.sos/utils.py'\n",
    "    import warnings\n",
    "    if ${gene_list.is_file()}:\n",
    "        gene_names = [x.strip() for x in open(${gene_list:r}).readlines() if x.strip()]\n",
    "    else:\n",
    "        gene_names = get_tb_grps([${_input:r,}])\n",
    "    if ${keep_ensg_version} == 0:\n",
    "        gene_names = [os.path.splitext(x)[0] for x in gene_names]\n",
    "    output = dict()\n",
    "    output['null'] = {'colnames': None, 'rownames': [], 'beta': None, 'se': None, 'pval': None}\n",
    "    output['max'] = {'colnames': None, 'rownames': [], 'beta': None, 'se': None, 'pval': None}\n",
    "    failure_ct = 0\n",
    "    for idx, name in enumerate(gene_names):\n",
    "        # extract the best gene-snp pair or some null gene-snp pairs\n",
    "        res = get_gs_pairs(TBData(${_input:r}, name), name, (1, ${null_per_gene}))\n",
    "        #\n",
    "        if res is None:\n",
    "            failure_ct += 1\n",
    "            continue\n",
    "        for k in output:\n",
    "            if not k in res:\n",
    "                continue\n",
    "            for kk in output[k]:\n",
    "                if kk == 'rownames':\n",
    "                    if output[k]['rownames'] is None:\n",
    "                        output[k]['rownames'] = res[\"{}_rownames\".format(k)]\n",
    "                    else:\n",
    "                        output[k]['rownames'].extend(res[\"{}_rownames\".format(k)])\n",
    "                elif kk == 'colnames':\n",
    "                    if output[k]['colnames'] is None:\n",
    "                        output[k]['colnames'] = res['colnames']\n",
    "                else:\n",
    "                    output[k][kk] = np.vstack((output[k][kk], res[k][kk])) if output[k][kk] is not None else res[k][kk]\n",
    "    #\n",
    "    if failure_ct < len(gene_names):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category = tb.FlavorWarning)\n",
    "            for k in output:\n",
    "                TBData(dict(output[k]), k, msg = \"%s, %s gene-snp pair\" % (\"${msg}\", k), complib = 'zlib').sink(${_output:r})\n",
    "    with open(\"${_output:n}.log\", 'w') as f:\n",
    "        f.write(\"%s out of %s groups extracted!\\n\" % (len(gene_names) - failure_ct, len(gene_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Z score and $\\hat{V}$ for MASH EZ model and save to RDS\n",
    "\n",
    "Here I also implemented two command options:\n",
    "\n",
    "- `--include_mash_effects_list`: a list of effect names (row names) to include from output MASH data-set. This option was used in MASH paper to remove SNPs in LD with each other.\n",
    "- `--include_mash_condition_list`: a list of conditions (column names) to include in output MASH data-set. Conditions not in this list are excluded from output. This option was used in MASH paper to focus analysis on brain / no-brain tissues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_4]\n",
    "# Compute for MASH EZ model and save to RDS\n",
    "depends: R_library('rhdf5')\n",
    "output: f\"{_input:n}.mash.rds\" if not str(_input).endswith('.portable.h5') else f\"{_input:nn}.mash.rds\"\n",
    "\n",
    "R: expand = \"${ }\"\n",
    "    ConvertP2Z <- function(pval, beta) {\n",
    "      z <- abs(qnorm(pval / 2))\n",
    "      z[which(beta < 0)] <- -1 * z[which(beta < 0)]\n",
    "      return(z)\n",
    "    }\n",
    "\n",
    "    GetSS <- function(table, db) {\n",
    "      dat <- rhdf5::h5read(db, table)\n",
    "      dat$\"z\" <- ConvertP2Z(dat$\"pval\", dat$\"beta\")\n",
    "      for (name in c(\"beta\", \"se\", \"pval\", \"z\")) {\n",
    "        dat[[name]] <- t(dat[[name]])\n",
    "        colnames(dat[[name]]) <- dat$colnames\n",
    "        rownames(dat[[name]]) <- dat$rownames\n",
    "      }\n",
    "      dat$colnames <- dat$rownames <- NULL\n",
    "      return(dat)\n",
    "    }\n",
    "  \n",
    "    SplitTrainTest <- function(dat, table) {\n",
    "        # load data\n",
    "        mdat = dat$max[[table]]\n",
    "        ndat = dat$null[[table]]\n",
    "        # select rows to keep\n",
    "        num_train = ${mash_train_size}\n",
    "        if (num_train >= nrow(ndat)) {\n",
    "            num_train = floor(nrow(ndat) / 2)\n",
    "        }\n",
    "        train = ndat[1:num_train,]\n",
    "        validate = ndat[(num_train+1):nrow(ndat),]\n",
    "        if (${include_mash_effects_list:r} != 'NULL') {\n",
    "            pout = scan(${include_mash_effects_list:ar}, what=\"character\", sep=NULL)\n",
    "            mdat = mdat[(rownames(mdat) %in% pout),]\n",
    "            ndat = ndat[(rownames(ndat) %in% pout),]\n",
    "            train = train[(rownames(train) %in% pout),]\n",
    "            validate = validate[(rownames(validate) %in% pout),]                                         \n",
    "        }\n",
    "        if (${include_mash_condition_list:r} != 'NULL') {\n",
    "            rout = scan(${include_mash_condition_list:ar}, what=\"character\", sep=NULL)\n",
    "            mdat = mdat[,(colnames(mdat) %in% rout),drop=F]\n",
    "            ndat = ndat[,(colnames(ndat) %in% rout),drop=F]\n",
    "            train = train[,(colnames(train) %in% rout),drop=F]\n",
    "            validate = validate[,(colnames(validate) %in% rout),drop=F]\n",
    "        } \n",
    "        # get vhat (SVS)\n",
    "        vhat = NULL\n",
    "        if (table == 'z') {\n",
    "            max_absz = apply(abs(ndat),1, max)\n",
    "            nullish = which(max_absz < 2)\n",
    "            nz = ndat[nullish,,drop=F]\n",
    "            vhat = cor(nz)\n",
    "        }\n",
    "        return(list(train = train,\n",
    "               validate = validate, \n",
    "               test = mdat, vhat = vhat))\n",
    "    }\n",
    "      \n",
    "    SS_data = list(max = GetSS('max', ${_input:r}), null = GetSS('null', ${_input:r}))\n",
    "    ztable = SplitTrainTest(SS_data, \"z\")\n",
    "    btable = SplitTrainTest(SS_data, \"beta\")\n",
    "    stable = SplitTrainTest(SS_data, \"se\")\n",
    "    # save output\n",
    "    saveRDS(list(train.z = ztable$train,\n",
    "                 validate.z = ztable$validate,\n",
    "                 test.z = ztable$test,\n",
    "                 train.b = btable$train,\n",
    "                 validate.b = btable$validate,\n",
    "                 test.b = btable$test,\n",
    "                 train.s = stable$train,\n",
    "                 validate.s = stable$validate,\n",
    "                 test.s = stable$test,\n",
    "                 vhat = ztable$vhat), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export this workflow to HTML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[export]\n",
    "# Export notebook to HTML file\n",
    "input: [item for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 1\n",
    "output: [(f'{cwd:a}/{item:bn}.full.html', f'{cwd:a}/{item:bn}.lite.html') for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 2\n",
    "bash: expand = True, stderr = False\n",
    "  sos convert {_input} {_output[0]}\n",
    "  sos convert {_input} {_output[1]} --template sos-report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.9.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
