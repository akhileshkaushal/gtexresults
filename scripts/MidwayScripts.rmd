## Data files required to reproduce this analysis can be downloaded at
## https://github.com/surbut/gtexresults_matrixash/wiki/mash_gtex_analysis.zip. They are also avialble in the directory Inputs. Conveninetly, the output of these files have been provided in Data_Results.

We assume you put these files in Input. First. we need to compute the deconvolved covariance matrices using the ExtremeDeconvolution package. This step may take some time, so we suggest to run on a cluster. We deconvolve the estimates below:
  


```{r}
library('mashr')
library('ExtremeDeconvolution')

t.stat=read.table("../Inputs/maxz.txt")

mean.mat=matrix(rep(0,ncol(t.stat)*nrow(t.stat)),ncol=ncol(t.stat),nrow=nrow(t.stat))
lambda.mat=as.matrix(read.table("../Inputs/zsfa_lambda.out"))
factor.mat=as.matrix(read.table("../Inputs/zsfa_F.out"))



s.j=matrix(rep(1,ncol(t.stat)*nrow(t.stat)),ncol=ncol(t.stat),nrow=nrow(t.stat))


#max_absz = apply(abs(ndat$z),1, max)
#nullish = which(max_absz < 2)
#nz = dat$z[nullish,]
#vhat = cor(nz)
#saveRDS(vhat,file = "vhat.RDS")

v.mat=readRDS("../Inputs/vhat.RDS")

v.j=list()
for(i in 1:nrow(t.stat)){v.j[[i]]=v.mat}
mean.mat=matrix(rep(0,ncol(t.stat)*nrow(t.stat)),ncol=ncol(t.stat),nrow=nrow(t.stat))
lambda.mat=as.matrix(read.table("~//jul3/zsfa_lambda.out"))
factor.mat=as.matrix(read.table("~//jul3/zsfa_F.out"))
permsnp=10
K=3;P=3;R=44
init.cov=init.covmat(t.stat=t.stat,factor.mat = factor.mat,lambda.mat = lambda.mat,K=K,P=P)
init.cov.list=list()
for(i in 1:K){init.cov.list[[i]]=init.cov[i,,]}
head(init.cov.list)

ydata=  t.stat
xamp= rep(1/K,K)
xcovar= init.cov.list
fixmean= TRUE     
ycovar=  v.j     
xmean=   mean.mat   
projection= list();for(l in 1:nrow(t.stat)){projection[[l]]=diag(1,R)}

e=extreme_deconvolution(ydata=ydata,ycovar=ycovar,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)

true.covs=array(dim=c(K,R,R))
for(i in 1:K){true.covs[i,,]=e$xcovar[[i]]}
pi=e$xamp

max.step=list(true.covs=true.covs,pi=pi)
saveRDS(max.step,paste0("max.steps3033",P,".rds"))

```



After denoising, then we run MASH. First we used the denoised estimates as well as the canonical configurations and single rnak SFA estimates to create a list of covariance matrices:

```{r}

max.step=readRDS("max.steps3033.rds")
z.stat=read.table("../Inputs/maxz.txt")
rownames(z.stat)=NULL
colnames(z.stat)=NULL
v.mat=readRDS("../Inputs/vhat.RDS")

v.j=matrix(rep(1,ncol(z.stat)*nrow(z.stat)),ncol=ncol(z.stat),nrow=nrow(z.stat))



lambda.mat=as.matrix(read.table("../zsfa_lambda.out"))
factor.mat=as.matrix(read.table("../zsfa_F.out"))

A="withvhat"

covmat=compute.hm.covmat.all.max.step(b.hat=z.stat,se.hat=v.j,t.stat=z.stat,Q=5,lambda.mat,A=A,factor.mat=factor.mat,max.step=max.step,zero=TRUE)$covmat
```


Then, we compute the hierarhcical weights using a training set:

```{r computehm weights}
train.z=as.matrix(read.table("../Inputs/trainz.txt"))

rownames(train.z)=NULL
colnames(train.z)=NULL
train.v=train.z/train.z



compute.hm.train.log.lik.pen.vmat(train.b = train.z,covmat = covmat,A = A,pen = 1,train.s = train.v,cormat = v.mat)

pis=readRDS("../Inputs/piswithvhat.rds")$pihat
z.stat=read.table("../Inputs/maxz.txt")

tim=proc.time()
weightedquants=lapply(seq(1:nrow(z.stat)),function(j){
  total.quant.per.snp.with.vmat(j=j, covmat=covmat, b.gp.hat=z.stat, cormat=v.mat, se.gp.hat=v.j, pis=pis, A=A, checkpoint = FALSE)})
tim-proc.time()
```

This will produce 6 files of posteriors `withvhatposterior*.txt` as well as a matrix of likelihoods (`liketrainwithvhat.rds` and hierarchical weights (`piswithvhat.rds`)).

